# -*- coding: utf-8 -*-
"""LIG-doctor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O8oM2yqf0czspNtX3f2ojFBJGYby1-jc
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from transformers import BertModel, BertTokenizer
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence

# Preprocessing Function
def preprocess_train_set(train_set, number_of_input_codes):
    num_patients = len(train_set)
    max_visits = max(len(patient) for patient in train_set)

    x_hotvectors = [torch.zeros((len(patient), number_of_input_codes)) for patient in train_set]
    clinical_notes = []

    for i, patient in enumerate(train_set):
        patient_notes = []
        for j, visit in enumerate(patient):
            for code in visit[0]:
                x_hotvectors[i][j, code] = 1
            patient_notes.append(visit[1])
        clinical_notes.append(patient_notes)

    return x_hotvectors, clinical_notes

# Custom Dataset
class PatientDataset(Dataset):
    def __init__(self, x_hotvectors, clinical_notes):
        self.x_hotvectors = x_hotvectors
        self.clinical_notes = clinical_notes

    def __len__(self):
        return len(self.x_hotvectors)

    def __getitem__(self, index):
        return self.x_hotvectors[index], self.clinical_notes[index]

# Custom Collate Function
def collate_fn(batch):
    x_hotvectors, clinical_notes = zip(*batch)

    # Pad x_hotvectors
    x_padded = pad_sequence(x_hotvectors, batch_first=True)

    # Pad clinical notes
    max_seq_len = max(len(notes) for notes in clinical_notes)
    clinical_notes_padded = [
        notes + [""] * (max_seq_len - len(notes)) for notes in clinical_notes
    ]

    seq_lengths = torch.tensor([len(notes) for notes in clinical_notes])

    return x_padded, clinical_notes_padded, seq_lengths

# Model Definition
class MinGRUBERTModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, bert_model_name="bert-base-uncased", num_layers=1):
        super(MinGRUBERTModel, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        self.bert = BertModel.from_pretrained(bert_model_name)
        self.bert_hidden_dim = self.bert.config.hidden_size
        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)

        self.gru = nn.GRU(input_dim + self.bert_hidden_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, input_dim)

    def encode_clinical_notes(self, clinical_notes, device):
        flattened_notes = [note for notes in clinical_notes for note in notes]
        tokenized = self.tokenizer(flattened_notes, padding=True, truncation=True, return_tensors="pt").to(device)
        with torch.no_grad():
            bert_output = self.bert(**tokenized)
        bert_embeddings = bert_output.last_hidden_state[:, 0, :]

        batch_size = len(clinical_notes)
        max_seq_len = max(len(notes) for notes in clinical_notes)
        reshaped_embeddings = torch.zeros((batch_size, max_seq_len, self.bert_hidden_dim), device=device)
        idx = 0
        for i, notes in enumerate(clinical_notes):
            for j in range(len(notes)):
                reshaped_embeddings[i, j] = bert_embeddings[idx]
                idx += 1

        return reshaped_embeddings

    def forward(self, x_hotvectors, clinical_notes, seq_lengths):
        device = x_hotvectors.device
        bert_embeddings = self.encode_clinical_notes(clinical_notes, device)

        gru_input = torch.cat((x_hotvectors, bert_embeddings), dim=2)
        packed_input = pack_padded_sequence(gru_input, seq_lengths.cpu(), batch_first=True, enforce_sorted=False)

        h0 = torch.zeros(self.num_layers, x_hotvectors.size(0), self.hidden_dim).to(device)
        packed_output, _ = self.gru(packed_input, h0)

        unpacked_output, _ = pad_packed_sequence(packed_output, batch_first=True)
        output = self.fc(unpacked_output)
        return output

# Training Function
def train_model(model, train_loader, criterion, optimizer, device):
    model.train()
    total_loss = 0.0
    for x_hotvectors, clinical_notes, seq_lengths in train_loader:
        x_hotvectors = x_hotvectors.to(device)
        seq_lengths = seq_lengths.to(device)

        outputs = model(x_hotvectors, clinical_notes, seq_lengths)
        loss = criterion(outputs, x_hotvectors)
        total_loss += loss.item()

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    return total_loss / len(train_loader)

# Testing Function
def test_model(model, test_loader, criterion, device):
    model.eval()
    total_loss = 0.0
    with torch.no_grad():
        for x_hotvectors, clinical_notes, seq_lengths in test_loader:
            x_hotvectors = x_hotvectors.to(device)
            seq_lengths = seq_lengths.to(device)

            outputs = model(x_hotvectors, clinical_notes, seq_lengths)
            loss = criterion(outputs, x_hotvectors)
            total_loss += loss.item()

    return total_loss / len(test_loader)

# Main Function
def main(train_set, test_set, input_dim, hidden_dim, batch_size, learning_rate, epochs):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    x_train, clinical_train = preprocess_train_set(train_set, input_dim)
    x_test, clinical_test = preprocess_train_set(test_set, input_dim)

    train_dataset = PatientDataset(x_train, clinical_train)
    test_dataset = PatientDataset(x_test, clinical_test)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

    model = MinGRUBERTModel(input_dim, hidden_dim).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    for epoch in range(epochs):
        train_loss = train_model(model, train_loader, criterion, optimizer, device)
        test_loss = test_model(model, test_loader, criterion, device)

        print(f"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}")

# Hyperparameters and Example Usage
input_dim = 270
hidden_dim = 128
batch_size = 16
learning_rate = 1e-4
epochs = 10

inputFileRadical = "data/preprocessing_271"
def load_data():
    global numberOfInputCodes
    main_trainSet = pickle.load(open(inputFileRadical+'.train', 'rb'))
    print("-> " + str(len(main_trainSet)) + " patients at dimension 0 for file: " + inputFileRadical + ".train dimensions ")
    main_testSet = pickle.load(open(inputFileRadical+'.test', 'rb'))
    print("-> " + str(len(main_testSet)) + " patients at dimension 0 for file: " + inputFileRadical + ".test dimensions ")
    print("Note: these files carry 3D tensor data; the above numbers refer to dimension 0, dimensions 1 and 2 have irregular sizes.")

    # numberOfInputCodes = getNumberOfCodes([main_trainSet, main_testSet])
    numberOfInputCodes = 270
    print('Number of diagnosis input codes: ' + str(numberOfInputCodes))

    train_sorted_index = sorted(range(len(main_trainSet)), key=lambda x: len(main_trainSet[x]))
    main_trainSet = [main_trainSet[i] for i in train_sorted_index]

    test_sorted_index = sorted(range(len(main_testSet)), key=lambda x: len(main_testSet[x]))
    main_testSet = [main_testSet[i] for i in test_sorted_index]

    return main_trainSet, main_testSet
train_set, test_set = load_data()

main(train_set, test_set, input_dim, hidden_dim, batch_size, learning_rate, epochs)

